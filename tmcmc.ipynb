{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsVR/CebjCjMCLxWDoZWdV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supsi-dacd-isaac/TeachDecisionMakingUncertainty/blob/main/tmcmc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal, norm\n",
        "from numpy.linalg import cholesky\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import logging"
      ],
      "metadata": {
        "id": "g-MQEqq6Q4-Q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "efqoN7kLGCd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute beta and weights\n",
        "def _beta_and_weights(beta, adjusted_likelihood):\n",
        "    low = beta\n",
        "    high = 2\n",
        "\n",
        "    while (high - low) / ((high + low) / 2) > 1e-6 and high > np.finfo(float).eps:\n",
        "        x = (high + low) / 2\n",
        "        w = np.exp((x - beta) * adjusted_likelihood)\n",
        "\n",
        "        if np.std(w) / np.mean(w) > 1:\n",
        "            high = x\n",
        "        else:\n",
        "            low = x\n",
        "\n",
        "    return min(1, x), w"
      ],
      "metadata": {
        "id": "YdjZr2SEGDsR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k12upFAhGDUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate proposals\n",
        "def proprnd(mu, cov_matrix, prior):\n",
        "    while True:\n",
        "        sample = multivariate_normal.rvs(mean=mu, cov=cov_matrix)\n",
        "        if not np.isinf(prior(sample)):\n",
        "            return sample"
      ],
      "metadata": {
        "id": "7Ccg_vWVscJY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ie5jO_2oGM3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run Metropolis-Hastings\n",
        "def metropolis_hastings_simple(target, proposal, x0, n_samples, burnin=10, thin=3):\n",
        "    samples = []\n",
        "    x = x0\n",
        "    for _ in range(n_samples * thin + burnin):\n",
        "        x_new = proposal(x)\n",
        "        acceptance_ratio = np.exp(target(x_new) - target(x))\n",
        "        if np.random.rand() < acceptance_ratio:\n",
        "            x = x_new\n",
        "        samples.append(x)\n",
        "    return np.array(samples)[burnin::thin]"
      ],
      "metadata": {
        "id": "3Nl8jciiGMU5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ttransitional MarkovChain MC. Why? How does it work?\n",
        "\n",
        "\n",
        "\n",
        "*   `M = f(x;θ)` Model class;\n",
        "*   `θ` = Parameter vector;\n",
        "*  ` D = {f1,...,fn} `the data;\n",
        "\n",
        "\n",
        "    Model updating:\n",
        "    Prob(θ|D,M) = Prob(D|θ,M) · Prob(θ|M) / Prob(D|M)\n",
        "\n",
        "    Prob(D|M) := evidence (normalizing constant)\n",
        "    Prob(θ|M) := prior on model params\n",
        "    Prob(D|θ,M) := likelihood of Data given model and model params\n",
        "    Prob(θ|D,M) := posterior on model params given Data and model class\n",
        "-\n",
        "\n",
        "    Approximated Bayesian Computation (AABC)\n",
        "\n",
        "    Prob(θ|D,M) \\propto Prob(D|θ,M) · Prob(θ|M)\n",
        "\n",
        "    Prob(D|θ,M):= pseudo likelihood\n",
        "$\\propto exp(-\\frac{d(D,M(\\theta))^2}{\\epsilon^2})$\n",
        "\n",
        "    Approximated Transitional updating:\n",
        "                Pj \\propto P(θ|M) · P(D|θ,M)^βj\n",
        "    βj: slowly update prior to the posterior\n",
        "-\n"
      ],
      "metadata": {
        "id": "lgT01iYkHYg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.covariance import LedoitWolf\n",
        "# Ledoit-Wolf shrinkage estimator\n",
        "lw = LedoitWolf()\n",
        "\n",
        "\n",
        "def tmcmc(log_fD_T, log_fT, sample_fT, Nsamples,  burnin=20, thin=3, beta2=0.01):\n",
        "    \"\"\"\n",
        "    Transitional Markov Chain Monte Carlo (TMCMC) method for Bayesian inference.\n",
        "    Adapted from Ching and Chen (2007)\n",
        "    % ------------------------------------------------------------------------\n",
        "    % who                    when         observations\n",
        "    %--------------------------------------------------------------------------\n",
        "    % Diego Andres Alvarez   Jul-24-2013  First algorithm (MATLAB)\n",
        "    %--------------------------------------------------------------------------\n",
        "    % Roberto Rocchetta      Dec-12-2024  Initial version (Python)\n",
        "    %--------------------------------------------------------------------------\n",
        "\n",
        "    Parameters:\n",
        "    - log_fD_T: log-likelihood (log-pdf or approximated likelihood prop to -distance(Data,Model)\n",
        "    - log_fT: the prior distribution (PDF)\n",
        "    - sample_fT: the prior distribution (sampler)\n",
        "\n",
        "    - Nsamples: Number of theta samples to generate.\n",
        "    - beta2: Scaling factor for adaptive proposal distribution.\n",
        "    - burnin: Number of burn-in steps for MCMC.\n",
        "\n",
        "    Returns:\n",
        "    - samples_posterior_pdf: Samples from the posterior distribution.\n",
        "    - log_evidence: Log of the Bayesian evidence.\n",
        "    \"\"\"\n",
        "    # Iteration number, Tempering parameter, Log Evidence\n",
        "    j1, beta_j, Log_ev = 0, 0, 0\n",
        "    theta_j = np.array(sample_fT(Nsamples))  # Samples of prior\n",
        "\n",
        "    while beta_j < 1:   ##START\n",
        "        j1 += 1\n",
        "\n",
        "        # Compute likelihood\n",
        "        Lp_j = np.array([log_fD_T(theta) for theta in theta_j])\n",
        "        Lp_adjust = np.max(Lp_j)\n",
        "\n",
        "        # Compute new beta and weights\n",
        "        logging.info(f\"'Computing beta the weights ...\")\n",
        "        beta_j1, w_j = _beta_and_weights(beta_j, Lp_j - Lp_adjust)\n",
        "        print(f\"Iteration {j1}: Tempering parameter updated to {beta_j1:.3f}\")\n",
        "\n",
        "        Log_ev += np.log(np.mean(w_j)) + (beta_j1 - beta_j) * Lp_adjust\n",
        "\n",
        "        # Weighted resampling of theta_j\n",
        "        wn_j = w_j / np.sum(w_j)  # Normalize weights\n",
        "        indices = np.random.choice(range(Nsamples), size=Nsamples, p=wn_j.flatten())\n",
        "        theta_j1 = theta_j[indices]\n",
        "\n",
        "        # Estimate covariance matrix\n",
        "        covariance = beta2 * lw.fit(theta_j1).covariance_\n",
        "        # covariance = beta2 * np.cov(theta_j1, rowvar=False)\n",
        "\n",
        "        # Define proposal and target functions\n",
        "        proposal = lambda mu: proprnd(mu, covariance, log_fT)\n",
        "        target = lambda x: beta_j1 * log_fD_T(x) + log_fT(x)\n",
        "\n",
        "        # Run Markov chains posterior sampling\n",
        "        theta_j1 = np.array([\n",
        "                    metropolis_hastings_simple(target, proposal, theta, 1, burnin=burnin, thin=thin)[0]\n",
        "                    for theta in tqdm(theta_j1, desc=\"Running MH - Markov Chains\")])\n",
        "\n",
        "        beta_j = beta_j1\n",
        "        theta_j = theta_j1\n",
        "\n",
        "    return theta_j, Log_ev"
      ],
      "metadata": {
        "id": "kzH9NpseGjYJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the Two Moons dataset\n",
        "def generate_two_moons(n_samples=100, noise=0.1):\n",
        "    theta = np.linspace(0, np.pi, n_samples // 2)\n",
        "    x1 = np.cos(theta)\n",
        "    y1 = np.sin(theta)\n",
        "\n",
        "    x2 = 1 - np.cos(theta)\n",
        "    y2 = -np.sin(theta)\n",
        "\n",
        "    x = np.concatenate([x1, x2])\n",
        "    y = np.concatenate([y1, y2])\n",
        "\n",
        "    x += noise * np.random.randn(n_samples)\n",
        "    y += noise * np.random.randn(n_samples)\n",
        "\n",
        "    return np.column_stack([x, y])\n",
        "\n",
        "# Generate outputs based on a quadratic function\n",
        "def model(xa, xe, xc):\n",
        "    a, b, c = xe[0], xe[1], xc[0]\n",
        "    x, y = xa[:, 0], xa[:, 1]\n",
        "\n",
        "    o1 = a * x ** 2 + b * y + c\n",
        "    o2 = a * y ** 2 + (b + c) * x + c\n",
        "    o3 = 1 / a * (b * y + c * x) ** 2\n",
        "    return np.array([o1, o2, o3]).T\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c3oUpyZHb25r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Generative model for  Xa (unknown stochastic/aleatory sources of uncertainty)\n",
        "\n",
        "    n_samples = 500\n",
        "    Xa = generate_two_moons(n_samples=n_samples, noise=0.05)\n",
        "\n",
        "    # Ture epistemic uncertain parameters, Xe (unknown but fixed in the system or model)\n",
        "    Xe_true = [2.0, -1.0]\n",
        "\n",
        "    # unknown model parameters, e.g., control, Xc\n",
        "    Xc_true = [0.5]\n",
        "\n",
        "    # generate data set of model outputs\n",
        "    Data_empirical = model(Xa, Xe_true, Xc_true)\n",
        "\n",
        "    sns.pairplot(pd.DataFrame(Data_empirical))\n",
        "    plt.show()\n",
        "\n",
        "    # unknown uncertain inputs\n",
        "    sns.pairplot(pd.DataFrame(Xa))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2GyntX6cscL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# __________________________    Define epistemic variables\n",
        "\n",
        "epistemic_domain = {\n",
        "                    'Xe_1': (+1.8, +2.2),  # X epistemic 1 (lower, upper)\n",
        "                    'Xe_2': (-1.4, -0.9),  # X epistemic 2 (lower, upper)\n",
        "                    'Xa1_mu1': (-1, 2),  # Xa mean var 1 Gaussian_mixture, comp 1 (lower, upper)\n",
        "                    'Xa1_mu2': (-1, 2),  # Xa mean var 1 Gaussian_mixture, comp 2 (lower, upper)\n",
        "                    'Xa1_mu3': (-1, 2),  # Xa mean var 1 Gaussian_mixture, comp 1 (lower, upper)\n",
        "                    'Xa1_mu4': (-1, 2),  # Xa mean var 1 Gaussian_mixture, comp 2 (lower, upper)\n",
        "                    'Xa2_mu1': (-1, 2),  # Xa mean var 2 Gaussian_mixture, comp 1 (lower, upper)\n",
        "                    'Xa2_mu2': (-1, 2),   # Xa mean var 2, Gaussian_mixture, comp 2 (lower, upper)\n",
        "                    'Xa2_mu3': (-1, 2),  # Xa mean var 2 Gaussian_mixture, comp 1 (lower, upper)\n",
        "                    'Xa2_mu4': (-1, 2),  # Xa mean var 2, Gaussian_mixture, comp 2 (lower, upper)\n",
        "                    'Xa1_std1': (0.1, 1),  # stds... same as above...assume diag covariance\n",
        "                    'Xa1_std2': (0.1, 1),  #\n",
        "                    'Xa1_std3': (0.1, 1),  #\n",
        "                    'Xa1_std4': (0.1, 1),  #\n",
        "                    'Xa2_std1': (0.1, 1),  #\n",
        "                    'Xa2_std2': (0.1, 1),  #\n",
        "                    'Xa2_std3': (0.1, 1),  #\n",
        "                    'Xa2_std4': (0.1, 1),  #\n",
        "                  }\n",
        "\n",
        "xe_indices = [0, 1]  # indices of the epistemic variables\n",
        "lb = np.array([var[0] for var in epistemic_domain.values()])\n",
        "ub = np.array([var[1] for var in epistemic_domain.values()])"
      ],
      "metadata": {
        "id": "KiiR5-89Mycu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define the black-box physical model M combined with the distributional Model Fa model for the inputs Xa\n",
        "\n",
        "# Define the black-box aleatory model (f_a)\n",
        "def my_Aleatroy_GM_model(params_dic, n_samples: int = 200):\n",
        "\n",
        "    np.random.seed(0)#do we need it? probably not\n",
        "\n",
        "    # Extract parameters for the Gaussian mixture\n",
        "    means = [\n",
        "        [params_dic['Xa1_mu1'], params_dic['Xa2_mu1']],\n",
        "        [params_dic['Xa1_mu2'], params_dic['Xa2_mu2']],\n",
        "        [params_dic['Xa1_mu3'], params_dic['Xa2_mu3']],\n",
        "        [params_dic['Xa1_mu4'], params_dic['Xa2_mu4']]\n",
        "    ]\n",
        "    stds = [\n",
        "        [params_dic['Xa1_std1'], params_dic['Xa2_std1']],\n",
        "        [params_dic['Xa1_std2'], params_dic['Xa2_std2']],\n",
        "        [params_dic['Xa1_std3'], params_dic['Xa2_std3']],\n",
        "        [params_dic['Xa1_std4'], params_dic['Xa2_std4']] # generalized to n comps?\n",
        "    ]\n",
        "\n",
        "    weights = [0.25, 0.25, 0.25, 0.25]  # assume equal weights for the Gaussian components\n",
        "\n",
        "    samples_xa = [] # Generate samples from each Gaussian component\n",
        "    for i, (mean, std) in enumerate(zip(means, stds)):\n",
        "        component_samples = np.random.normal(loc=mean, scale=std, size=(int(n_samples * weights[i]), len(mean)))\n",
        "        samples_xa.append(component_samples)\n",
        "\n",
        "    # Concatenate all samples\n",
        "    samples_xa = np.vstack(samples_xa) # np.random.shuffle(samples_xa)  # Shuffle the samples... not needed\n",
        "\n",
        "    return samples_xa\n",
        "\n",
        "\n",
        "def my_model(params, n_samples:int = 200):\n",
        "    # this computes the probabilistic response of the balck box model that uses the Fa model as probabilistic sampler\n",
        "    if not isinstance(params, list):\n",
        "        if params.ndim == 1:\n",
        "            params = params.reshape(1, -1)\n",
        "    Model_Responses_Pars = []\n",
        "    for par in params:\n",
        "        params_dic = {key: val for key, val in zip(epistemic_domain.keys(), par)}\n",
        "        samples_xa = my_Aleatroy_GM_model(params_dic, n_samples=n_samples)\n",
        "        Model_Responses_Pars.append(model(samples_xa, par[xe_indices], Xc_true))\n",
        "    return Model_Responses_Pars\n",
        "\n"
      ],
      "metadata": {
        "id": "qt4xZ662MOuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import wasserstein_distance, entropy, ks_2samp\n",
        "from scipy.stats import norm, multivariate_normal, uniform\n",
        "\n",
        "def pseudo_likelihood_fun(params, data, black_box_model,\n",
        "                          n_samples, epsilon_width_factor = 0.01,\n",
        "                          method='KL'):\n",
        "    \"\"\"\n",
        "    Compute a pseudo-likelihood based on divergence between data and model predictions.\n",
        "    Parameters:\n",
        "        data (array-like): Empirical target data.\n",
        "        black_box_model (function): Function that generates model predictions given params, control, and xa_samples.\n",
        "        params (array-like): Model parameters to evaluate.\n",
        "        method (str): Distance measure ('entropy', 'bhattacharyya', 'wasserstein').\n",
        "        n_samples (int): Number of samples to generate with the model.\n",
        "    Returns:\n",
        "        float: Pseudo-likelihood value.\n",
        "    \"\"\"\n",
        "\n",
        "    model_samples = black_box_model(params, n_samples) # Generate samples from the model\n",
        "    n_bins = 20  # Number of bins\n",
        "    scores = []\n",
        "    for mod_sam in model_samples:\n",
        "        data_hist, edges = np.histogramdd(data, bins=n_bins, density=True)\n",
        "        model_hist, _ = np.histogramdd(mod_sam, bins=edges, density=True)\n",
        "        # Flatten histograms for comparison + eps to avoid zero probabilities\n",
        "        epsilon = 1e-10\n",
        "        data_hist_flat = data_hist.flatten() + epsilon\n",
        "        model_hist_flat = model_hist.flatten() + epsilon\n",
        "        # Normalize to probability distributions\n",
        "        data_hist_flat /= np.sum(data_hist_flat)\n",
        "        model_hist_flat /= np.sum(model_hist_flat)\n",
        "        if method == 'KL':  # Compute Kullback-Leibler divergence (relative entropy)\n",
        "            scores.append(entropy(data_hist_flat, model_hist_flat))\n",
        "        elif method == 'WS':  # Compute Wasserstein distance (Earth mover's distance)\n",
        "            scores.append(wasserstein_distance(data_hist_flat, model_hist_flat))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {method}. Use 'KL', 'WS'.....\")\n",
        "\n",
        "    psd_likelihoods = -(np.array(scores) ** 2 / epsilon_width_factor ** 2) # try with loig likelihood\n",
        "    return psd_likelihoods\n",
        "\n",
        "# Define prior and likelihood\n",
        "class prior_uniform:\n",
        "    \"\"\" Define uniform prior distribution class\"\"\"\n",
        "    def __init__(self, lb, ub):\n",
        "        self.lbs = lb\n",
        "        self.ubs = ub\n",
        "        self.updfs = [uniform(lb, ub) for lb, ub in zip(self.lbs, self.ubs)]\n",
        "\n",
        "        # Validate bounds and input lengths\n",
        "        if len(self.lbs) != len(self.ubs):\n",
        "            raise ValueError(\"Mismatched lengths: 'lbs', 'ubs'\")\n",
        "        if any(a >= b for a, b in zip(self.lbs, self.ubs)):\n",
        "            raise ValueError(\"Each lower bound must be less than the corresponding upper bound.\")\n",
        "\n",
        "    def sample(self, N):\n",
        "        return [np.random.uniform(self.lbs, self.ubs) for _ in range(N)]\n",
        "\n",
        "    def pdf(self, thetas):\n",
        "        marginal_PDF_j = [uniform(lb, ub).pdf(x) for lb, ub, x in zip(self.lbs, self.ubs, thetas)]\n",
        "        return np.prod(marginal_PDF_j)\n",
        "\n",
        "    def log_pdf(self, thetas):\n",
        "        log_pdf = []\n",
        "        if isinstance(thetas, list):\n",
        "            for theta in thetas:\n",
        "                inside_bounds = np.logical_and(self.lbs <= theta, theta <= self.ubs)\n",
        "                if np.all(inside_bounds):\n",
        "                    log_pdf.append(-np.sum(np.log(self.ubs - self.lbs)))\n",
        "                else:\n",
        "                    log_pdf.append(-np.inf)\n",
        "        else:\n",
        "            inside_bounds = np.logical_and(self.lbs <= thetas, thetas <= self.ubs)\n",
        "            if np.all(inside_bounds):\n",
        "                log_pdf.append(-np.sum(np.log(self.ubs - self.lbs)))\n",
        "            else:\n",
        "                log_pdf.append(-np.inf)\n",
        "\n",
        "        return log_pdf\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EIIhGYL9KXm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# __________________________    Define the TMCMC model\n",
        "log_fD_T = lambda p: pseudo_likelihood_fun(params=p, data=Data_empirical,  black_box_model=my_model, epsilon_width_factor=0.1, n_samples=200, method='KL')\n",
        "log_fT = lambda x: prior_uniform(lb, ub).log_pdf(x)\n",
        "sample_fT = lambda n: prior_uniform(lb, ub).sample(n)\n",
        "\n",
        "N_sam = 200\n",
        "theta_j, Log_ev = tmcmc(log_fD_T, log_fT, sample_fT, N_sam, burnin=20, thin=3, beta2=0.01)\n",
        "\n"
      ],
      "metadata": {
        "id": "VoOAsu96KXgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(theta_j)\n",
        "# issue with repeated samples..."
      ],
      "metadata": {
        "id": "8_-HIkLOR11H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N=500\n",
        "\n",
        "params_dic = {key: val for key, val in zip(epistemic_domain.keys(), theta_j[0])}\n",
        "samples_xa_model = my_Aleatroy_GM_model(params_dic, n_samples=N)\n",
        "samples_y_model = my_model(theta_j[0], N)[0]\n",
        "\n",
        "\"\"\"for thet in theta_j[1:2]:\n",
        "  params_dic = {key: val for key, val in zip(epistemic_domain.keys(), thet)}\n",
        "\n",
        "  new_sam_xa = my_Aleatroy_GM_model(params_dic, n_samples=N)\n",
        "  print(np.shape(new_sam_xa))\n",
        "  samples_xa = np.vstack([samples_xa_model,new_sam_xa] )\n",
        "\"\"\"\n",
        "# Assuming `samples_xa` and `generate_two_moons` are already defined\n",
        "samples_xa_model = pd.DataFrame(samples_xa_model)\n",
        "samples_xa_model['type'] = 'posterior_xa'\n",
        "\n",
        "Xa = generate_two_moons(n_samples=N, noise=0.05)\n",
        "samples_true_model = pd.DataFrame(Xa)\n",
        "samples_true_model['type'] = 'true'\n",
        "\n",
        "\n",
        "samples_y_model = pd.DataFrame(samples_y_model)\n",
        "samples_y_model['type'] = 'posterior_y'\n",
        "\n",
        "samples_y_true = pd.DataFrame(model(Xa, Xe_true, Xc_true))\n",
        "samples_y_true['type'] = 'true'\n",
        "\n",
        "\n",
        "# Concatenate the two dataframes\n",
        "combined_samples_xa = pd.concat([samples_xa_model, samples_true_model], ignore_index=True)\n",
        "combined_samples_y = pd.concat([samples_y_model, samples_y_true], ignore_index=True)\n",
        "\n",
        "# Rename columns if necessary (seaborn pairplot requires clear variable names)\n",
        "combined_samples_xa.columns = [f\"feature_{i}\" for i in range(combined_samples_xa.shape[1] - 1)] + ['type']\n",
        "combined_samples_y.columns = [f\"feature_{i}\" for i in range(combined_samples_y.shape[1] - 1)] + ['type']\n",
        "\n",
        "# Plot using seaborn pairplot with hue\n",
        "sns.pairplot(combined_samples_xa, hue='type')\n",
        "plt.show()\n",
        "\n",
        "# Plot using seaborn pairplot with hue\n",
        "sns.pairplot(combined_samples_y, hue='type')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.scatter([th[0] for th in theta_j], [th[1] for th in theta_j])\n",
        "plt.scatter(Xe_true[0], Xe_true[1], 20, c='r')\n",
        "plt.xlim(lb[0], ub[0])\n",
        "plt.ylim(lb[1], ub[1])\n",
        "plt.xlabel('Xe_1')\n",
        "plt.ylabel('Xe_2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KqipNfq9KXdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bxwTYoJ_KXXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}